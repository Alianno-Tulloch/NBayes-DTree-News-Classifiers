{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12f1tZ6QDn78eBfVDEX3K3YpBw-8u4gsc","timestamp":1742334853942}],"mount_file_id":"12f1tZ6QDn78eBfVDEX3K3YpBw-8u4gsc","authorship_tag":"ABX9TyP8cIGtZh+jYYIGPoaBXgrq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Part 1 and 2: Importing Data, Filtering Words"],"metadata":{"id":"929y5n83Nl_-"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUpUmbhS_Kqz","executionInfo":{"status":"ok","timestamp":1742614392428,"user_tz":240,"elapsed":2259,"user":{"displayName":"Alianno","userId":"13535244749093801790"}},"outputId":"14982c8c-a419-48b5-f875-ef237620c91f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Training set size: (2286, 244)\n","Test set size: (980, 244)\n","\n","First 10 words in vocabulary: ['100' 'about' 'administration' 'after' 'again' 'against' 'ahead' 'all'\n"," 'america' 'american']\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","#Open the dataset files from Alianno's Google Drive\n","fake_headlines = \"/content/drive/MyDrive/School/CP322/Assignment_3/fake.txt\"\n","real_headlines = \"/content/drive/MyDrive/School/CP322/Assignment_3/real.txt\"\n","\n","def load_data():\n","  with open(real_headlines, 'r') as file_real:\n","    real_data = [line.strip() for line in file_real.readlines()] #Define's read_real as a list of the lines in real_headlines\n","\n","  with open(fake_headlines, 'r') as file_fake:\n","    fake_data = [line.strip() for line in file_fake.readlines()] #Define's read_fake as a list of the lines in fake_headlines\n","\n","  # Assign labels (1 for real, 0 for fake)\n","  real_labels = [1] * len(real_data)\n","  fake_labels = [0] * len(fake_data)\n","\n","  # Combine the real headlines and real labels into combined lists\n","  # all data saved in the same order, so that it matches up when it's trained\n","  all_data = real_data + fake_data\n","  all_labels = real_labels + fake_labels\n","\n","  # PART 2:\n","\n","  # Convert text to numerical features using CountVectorizer\n","  # max_df = 0.7: 70% upper limit\n","  # min_df = 0.005: 0.5% lower limit\n","  vectorizer = CountVectorizer(max_df = 0.7, min_df = 0.005)\n","  all_data_vector = vectorizer.fit_transform(all_data)  # Creates a sparse matrix (headlines Ã— unique words)\n","\n","  # Split into train (70%), test (30%) sets\n","  X_train, X_test, y_train, y_test = train_test_split(all_data_vector, all_labels, test_size=0.3, random_state=6)\n","\n","  return X_train, X_test, y_train, y_test, vectorizer\n","\n","# Run the function and load data\n","X_train, X_test, y_train, y_test, vectorizer = load_data()\n","\n","# Print some information about the processed data\n","print(f\"Training set size: {X_train.shape}\")\n","print(f\"Test set size: {X_test.shape}\")\n","print(\"\\nFirst 10 words in vocabulary:\", vectorizer.get_feature_names_out()[:10])  # Show first 10 words"]},{"cell_type":"markdown","source":["Part 3:"],"metadata":{"id":"CwNHyNk4WbKC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","def naive_bayes_classifier(X_train, X_test, y_train, y_test):\n","  # 1. Naive Bayes, train the model based on the training set\n","  model = MultinomialNB()\n","  model.fit(X_train, y_train)\n","\n","  accuracy = model.score(X_test, y_test)\n","  print(\"Accuracy:\", accuracy)\n","\n","\n","  # 2. Calcuate Posterior Probablilities\n","  probabilities = model.predict_proba(X_test)\n","  print(\"Posterior Probabilities (Using predict_proba):\\n\", probabilities)\n","  print(\"\\n\")\n","\n","  # 2.5. Manually running the posterior probability calculations\n","  prior_prob = model.class_log_prior_  # log P(y = c)\n","  feature_prob = model.feature_log_prob_  # log P(x_j | y = c)\n","\n","  prob_per_input = []\n","\n","  for sample in X_test:\n","    sample_post = prior_prob + sample @ feature_prob.T\n","\n","    sample_posterior = np.exp(sample_post)\n","\n","    # Normalize to sum to 1\n","    sample_posterior /= np.sum(sample_posterior)\n","\n","    prob_per_input.append(sample_posterior)\n","\n","  # Convert list to NumPy array\n","  prob_per_input = np.array(prob_per_input)\n","\n","  print(\"Manually calculated Posterior Probabilities:\\n\", prob_per_input)\n","  print(\"\\n\")\n","\n","\n","  # 3. Predict the Label of the test set of data\n","  y_pred = model.predict(X_test)\n","  print(\"Predicted Labels:\\n\", y_pred)\n","  print(\"\\n\")\n","\n","  # 4. Actual Label of the test data\n","  y_actual = np.array(y_test)\n","  print(\"Actual Labels:\\n\", y_actual)\n","  print(\"\\n\")\n","\n","  # 5. CONFUSION MATRIX\n","  cm = confusion_matrix(y_test, y_pred)\n","  print(\"Confusion Matrix Syntax:\")\n","  print(\"[[True Negatives, False Positives]\\n[False Negatives, True Positives]]\\n\")\n","  print(\"Confusion Matrix of the Naive Bayes Classifier:\\n\", cm)\n","  print(\"\\n\")\n","  return accuracy\n","\n","accuracy = naive_bayes_classifier(X_train, X_test, y_train, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vUnSN7wWeAE","executionInfo":{"status":"ok","timestamp":1742614397045,"user_tz":240,"elapsed":114,"user":{"displayName":"Alianno","userId":"13535244749093801790"}},"outputId":"c770f660-bf07-4c82-a9b8-7328687dffc9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8040816326530612\n","Posterior Probabilities (Using predict_proba):\n"," [[8.06484946e-01 1.93515054e-01]\n"," [5.00026862e-02 9.49997314e-01]\n"," [2.62518551e-04 9.99737481e-01]\n"," ...\n"," [9.71397939e-01 2.86020609e-02]\n"," [1.61395717e-01 8.38604283e-01]\n"," [1.30145806e-01 8.69854194e-01]]\n","\n","\n","Manually calculated Posterior Probabilities:\n"," [[[8.06484946e-01 1.93515054e-01]]\n","\n"," [[5.00026862e-02 9.49997314e-01]]\n","\n"," [[2.62518551e-04 9.99737481e-01]]\n","\n"," ...\n","\n"," [[9.71397939e-01 2.86020609e-02]]\n","\n"," [[1.61395717e-01 8.38604283e-01]]\n","\n"," [[1.30145806e-01 8.69854194e-01]]]\n","\n","\n","Predicted Labels:\n"," [0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0\n"," 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0\n"," 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 1\n"," 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1\n"," 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 0\n"," 0 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1\n"," 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1\n"," 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 1 0 0\n"," 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0\n"," 0 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1\n"," 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n"," 1 0 1 0 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 1\n"," 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0\n"," 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 0 0 1\n"," 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 0\n"," 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0\n"," 0 1 0 0 0 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 0\n"," 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 0 0 0\n"," 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0\n"," 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1\n"," 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1\n"," 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 1 1\n"," 0 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0\n"," 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1\n"," 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0\n"," 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0\n"," 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1]\n","\n","\n","Actual Labels:\n"," [0 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 1 0\n"," 0 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 1\n"," 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1\n"," 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1\n"," 1 1 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 0 0\n"," 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 1\n"," 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0\n"," 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1 0 0\n"," 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0\n"," 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1\n"," 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 1\n"," 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0\n"," 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0\n"," 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1\n"," 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0\n"," 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 0 0\n"," 0 0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1\n"," 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 0 0\n"," 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0\n"," 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1\n"," 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1\n"," 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1\n"," 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1\n"," 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1\n"," 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0\n"," 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0\n"," 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1]\n","\n","\n","Confusion Matrix Syntax:\n","[[True Negatives, False Positives]\n","[False Negatives, True Positives]]\n","\n","Confusion Matrix of the Naive Bayes Classifier:\n"," [[311 106]\n"," [ 86 477]]\n","\n","\n"]}]}]}